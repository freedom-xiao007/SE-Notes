# 分布式服务概览
***
## 基础
RPC相关的知识

RPC 原理
RPC的简化版原理如下图。
核心是代理机制。
1.本地代理存根: Stub
2.本地序列化反序列化
3.网络通信
4.远程序列化反序列化
5.远程服务存根: Skeleton
6.调用实际业务服务
7.原路返回服务结果
8.返回给本地调用方
注意处理异常。

RPC 原理 --1. 设计
RPC是基于接口的远程服务调用。
本地应用程序 与 远程应用程序，分别需要共享什么信息，角色有什么不同？
共享：POJO实体类定义，接口定义。
REST/PB下，真的不需要嘛？另一种选择：WSDL/WADL/IDL
远程->服务提供者，本地->服务消费者。

RPC 原理 --2. 代理
RPC是基于接口的远程服务调用。
Java下，代理可以选择动态代理，或者AOP实现
- C#直接有远程代理
- Flex可以使用动态方法和熟悉

RPC 原理 --3. 序列化
序列化和反序列化的选择：
1、语言原生的序列化，RMI，Remoting
2、二进制平台无关，Hessian，avro，kyro，fst等
3、文本，JSON、XML等

RPC 原理 --4. 网络传输
最常见的传输方式：
- TCP/SSL
- HTTP/HTTPS

RPC 原理 --5. 查找实现类
通过接口查找服务端的实现类。
一般是注册方式，
例如 dubbo 默认将接口和实现类配置到Spring

- [RPC原理]()
- [RPC Dome 实现]()

## 微服务架构
在上面的服务日益增多的情况，管理及相关调用变的复杂和麻烦，则走向了微服务架构

从 RPC 走向服务化 -> 微服务架构
具体的分布式业务场景里，除了能够调用远程方法，我们还需要考虑什么？
1、多个相同服务如何管理？
2、服务的注册发现机制？
3、如何负载均衡，路由等集群功能？
4、熔断，限流等治理能力。
5、重试等策略
6、高可用、监控、性能等等。

什么时候进行解耦拆分
- 研发速度
  - 交付效率低下
  - 团队开发相互依赖，无法独立交付
  - 新人上手慢
- 扩展
  - 无法再进行垂直扩展
  - 系统某部分需要独立扩展
- 部署
  - 单体发布慢、复杂、风险高
  - 某些需要进行独立部署

微服务拆分基本步骤：
- 1.新建新服务
- 2.链接调用新服务
- 3.DB拆分
  - 双写迁移DB：四个具体步骤，周期也可能比较长（28）
  - 数据分发去join

### Dubbo框架

Apache Dubbo 是一款高性能、轻量级的开源 Java 服务框架
六大核心能力：
面向接口代理的高性能RPC调用，智能容错和负载均衡，服务自动注册和发现，高度可
扩展能力，运行期流量调度，可视化的服务治理与运维。

整体架构
一张整体架构图很重要，源码分析和研究需要对照这进行参考
1. config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类
2. proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以ServiceProxy 为中心，扩展接口为 ProxyFactory
3. registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为RegistryFactory, Registry, RegistryService
4. cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance
5. monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为MonitorFactory, Monitor, MonitorService
6. protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为Protocol, Invoker, Exporter
7. exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer
8. transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为Channel, Transporter, Client, Server, Codec
9. serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput,ObjectOutput, ThreadPool

SPI 的应用
SPI 与 API
ServiceLoader机制
META-INF/dubbo/接口全限定名，文件内容为实现类（ShardingSphere使用）
其他两个类似的机制：Callback与EventBus
Dubbo的SPI扩展，最关键的SPI：Protocol
xxx=com.alibaba.xxx.XxxProtocol
启动时装配，并缓存到ExtensionLoader中。



第 18 课 4. Dubbo最佳实践
开发分包
建议将服务接口、服务模型、服务异常等均放在 API 包中，因为服务模型和异常也是API 的一部分，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。
服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。
服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。
不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。

环境隔离与分组
怎么做多环境的隔离？
1、部署多套？
2、多注册中心机制
3、group机制
4、版本机制
服务接口增加方法，或服务模型增加字段，可向后兼容，删除方法或删除字段，将不兼容，枚举类型新增字段也不兼容，需通过变更版本号升级。

参数配置
通用参数以 consumer 端为准，如果consumer端没有设置，使用provider数值
建议在 Provider 端配置的 Consumer 端属性有：
timeout：方法调用的超时时间
retries：失败重试次数，缺省是 2 2
loadbalance：负载均衡算法 3，缺省是随机 random。
actives：消费者端的最大并发调用限制，即当 Consumer 对一个服务的并发调用到上限后，新
调用会阻塞直到超时，可以配置在方法或服务上。
建议在 Provider 端配置的 Provider 端属性有：
threads：服务线程池大小
executes：一个服务提供者并行执行请求上限，即当 Provider 对一个服务的并发调用达到上限后，新调用会阻塞，此时 Consumer 可能会超时。可以配置在方法或服务上。

容器化部署
注册的IP问题，两个解决办法：
1、docker使用宿主机网络
docker xxx -net xxxxx
2、docker参数指定注册的IP和端口, -e
DUBBO_IP_TO_REGISTRY — 注册到注册中心的IP地址
DUBBO_PORT_TO_REGISTRY — 注册到注册中心的端口
DUBBO_IP_TO_BIND — 监听IP地址
DUBBO_PORT_TO_BIND — 监听端口

运维与监控
Admin功能较简单，大规模使用需要定制开发，整合自己公司的运维监控系统。
可观测性：tracing、metrics、logging
- APM
- Promethus+Grafana

分布式事务
柔性事务，SAGA、TCC、AT
- Seata
- hmily
不支持 XA

重试与幂等
服务调用失败默认重试2次，如果接口不是幂等的，会造成业务重复处理。
如何设计幂等接口？
1、去重
2、类似乐观锁机制

- [Dubbo 简介]()

- [Dubbo 相关源码解析]()

### 分布式服务进阶
#### 分布式服务治理
分布式服务化与 SOA/ESB 的区别，体现分布式服务化的特点：服务之间是能相互调用的

#### 配置/注册/元数据中心
配置、注册、元数据，有何异同？
配置中心（ConfigCenter）：管理系统需要的配置参数信息
注册中心（RegistryCenter）：管理系统的服务注册、提供发现和协调能力
元数据中心（MetadataCenter）：管理各个节点使用的元数据信息
相同点：都需要保存和读取数据/状态，变更通知
不同点：配置是全局非业务参数，注册中心是运行期临时状态，元数据是业务模型

为什么会需要配置中心？
想想看：
1、大规模集群下，如何管理配置信息，特别是批量更新问题。
2、大公司和金融行业，一般要求开发、测试、运维分离（物理隔离）。
3、运行期的一些开关控制，总不能不断重启？？
Zookeeper、etcd、Nacos、Apollo。。。

为什么会需要注册中心？
有什么办法，让消费者能动态知道生产者集群的状态变化？
1、hello.htm -> ok
2、DNS？VIP？
3、主动报告+心跳
这些信息很重要，后续的集群管控，分布式服务治理，都要靠这个全局状态。

为什么会需要元数据中心？
一般情况下，没有问题也不大。
有了更好。
元数据中心，定义了所有业务服务的模型。

#### 服务的注册与发现
服务注册
服务提供者启动时，
- 将自己注册到注册中心（比如zk实现）的临时节点。
- 停止或者宕机时，临时节点消失。

注册的数据格式
- 节点key，代表当前服务（或者服务+版本）
- 多个子节点，每一个为一个提供者的描述信息

服务发现
服务消费者启动时，
- 从注册中心代表服务的主节点拿到多个代表提供者的临时节点列表，并本地缓存
（why？？？）。
- 根据router和loadbalance算法从其中的某一个执行调用。
- 如果可用的提供者集合发生变化时，注册中心通知消费者刷新本地缓存的列表。
例如zk可以使用curator作为客户端操作。

#### 服务的集群与路由
服务路由（ Service Route ）
跟网关的路由一样
1、比如基于IP段的过滤，
2、再比如服务都带上tag，用tag匹配这次调用范围。

服务负载均衡 （ Service LoadBalance ）
跟Nginx的负载均衡一样。
多个不同策略，原理不同，目的基本一致（尽量均匀）：
1、Random（带权重）
2、RoundRobin（轮询）
3、LeastActive（快的多给）
4、ConsistentHashLoadBalance（同样参数请求到一个提供者）

#### 服务的过滤与流控
服务过滤
所有的复杂处理，都可以抽象为管道+过滤器模式（Channel+Filter）
这个机制是一个超级bug的存在，
可以用来实现额外的增强处理（类似AOP），也可以中断当前处理流程，返回特定数据。
对比考虑一下，我们NIO网关时的filter，servlet的filter等。

为什么需要服务流控（ Flow Control ）
稳定性工程：
1、我们逐渐意识到一个问题：系统会故障是正常现象，就像人会生病
2、那么在系统出现问题时，直接不服务，还是保持部分服务能力呢？
系统的容量有限。
保持部分服务能力是最佳选择，然后在问题解决后恢复正常状态。
响应式编程里，这就是所谓的回弹性（Resilient）。
需要流控的本质原因是，输入请求大于处理能力。

服务流控
流控有三个级别：
1、限流（内部线程数，外部调用数或数据量）
2、服务降级（去掉不必要的业务逻辑，只保留核心逻辑）
3、过载保护（系统短时间不提供新的业务处理服务，积压处理完后再恢复输入请求）

### 微服务架构
#### 微服务架构发展历程*
微服务架构图
响应式微服务
服务网格与云原生
数据库网格
单元化架构

#### 微服务架构应用场景*
一张企业架构图：依据自己的情况，进行操作

怎么应用微服务架构 -I6I
调研-分析-规划-组织-拆分-部署-治理-改进

#### 微服务架构最佳实践*

#### Spring Cloud技术体系*

#### 微服务相关框架与工具
APM
OpenTelemetry

相关工具- - 权限控制
最核心的3A（其他：资源管理、安全加密等）：
- Authc: Authentication
- Authz: Authorization
- Audit
CAS+SSO(TGT、ST)
JWT/Token, OAuth2.0
SpringSecurity, Apache Shiro

相关工具- - 数据处理
1、读写分离与高可用HA：
2、分库分表Sharding：
3、分布式事务DTX：
4、数据迁移Migration：
5、数据集群扩容Scaling：
6、数据操作审计Audit：

相关工具- - 网关与通信
1、流量网关与WAF(Nginx/OR/Kong/Apisix)
2、业务网关(Zuul/Zuul2/SCG)
3、REST与其他协议之争（websocket/actor/rsocket/mq...）